{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d997217-0b33-4f82-86a9-e9c731d951a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"..\", \"training\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from training import VideoCNN, train_video_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bb354e0-2510-4c99-be60-94a0e04034db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle as pk\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d8baaeb-f2ca-499e-b22b-9e6038331e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_example(model_path, example_paths: list, verbose=False):\n",
    "    \"\"\"\n",
    "    Load a pre-trained model and predict the class for a given example\n",
    "    \"\"\"\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=\"cpu\")\n",
    "\n",
    "    # Create model\n",
    "    num_classes = checkpoint[\"num_classes\"]\n",
    "    model = VideoCNN(num_classes=num_classes)\n",
    "\n",
    "    # Load model\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "\n",
    "    # Predict\n",
    "    predictions = []\n",
    "\n",
    "    # Iterate over the list of example paths\n",
    "    for example_path in example_paths:\n",
    "        example_video = torch.from_numpy(train_video_processor(example_path)).unsqueeze(\n",
    "            0\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            outputs = model(example_video)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predicted_class = predicted.item()\n",
    "\n",
    "        # Print results\n",
    "        template_id = checkpoint[\"unique_templates\"][predicted_class]\n",
    "        predictions.append((example_path, int(template_id)))\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Model Path: {model_path}\")\n",
    "            print(f\"Example Path: {example_path}\")\n",
    "            print(f\"Predicted Label: {template_id}\")\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def predict_example_cuda(model_path, example_paths: list, verbose=False):\n",
    "    \"\"\"\n",
    "    Load a pre-trained model and predict the classes for a list of examples.\n",
    "    Utilizes GPU if available.\n",
    "    \"\"\"\n",
    "    # Check if GPU is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "    # Create model\n",
    "    num_classes = checkpoint[\"num_classes\"]\n",
    "    model = VideoCNN(num_classes=num_classes)\n",
    "\n",
    "    # Load model\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.to(device)  # Move model to the appropriate device\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare predictions list\n",
    "    predictions = []\n",
    "\n",
    "    # Iterate over the list of example paths\n",
    "    for example_path in example_paths:\n",
    "        # Prepare example input\n",
    "        example_video = torch.from_numpy(train_video_processor(example_path)).unsqueeze(\n",
    "            0\n",
    "        )\n",
    "        example_video = example_video.to(\n",
    "            device\n",
    "        )  # Move input data to the appropriate device\n",
    "\n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            outputs = model(example_video)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predicted_class = predicted.item()\n",
    "\n",
    "        # Map class to template ID\n",
    "        template_id = checkpoint[\"unique_templates\"][predicted_class]\n",
    "        predictions.append((example_path, int(template_id)))\n",
    "\n",
    "        # Print results if verbose\n",
    "        if verbose:\n",
    "            print(f\"Example Path: {example_path}\")\n",
    "            print(f\"Predicted Label: {template_id}\")\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def get_frame(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Check if the video was successfully opened\n",
    "    if not cap.isOpened():\n",
    "        raise Exception(\"Error: Could not open video.\")\n",
    "    else:\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        if total_frames == 0:\n",
    "            raise Exception(\"Error: Video contains no frames.\")\n",
    "\n",
    "        # Calculate the middle frame index\n",
    "        middle_frame_index = total_frames // 2\n",
    "\n",
    "        # Set the position to the middle frame\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, middle_frame_index)\n",
    "\n",
    "        # Read the middle frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "    cap.release()\n",
    "    return frame\n",
    "\n",
    "\n",
    "def display_and_export_images(list1, list2, output_pdf, pairs_per_page=2):\n",
    "    \"\"\"\n",
    "    Displays two lists of images side by side and exports them to a PDF.\n",
    "\n",
    "    Parameters:\n",
    "    - list1: List of images as NumPy arrays (first column).\n",
    "    - list2: List of images as NumPy arrays (second column).\n",
    "    - output_pdf: Path for the output PDF file.\n",
    "    - pairs_per_page: Number of image pairs to display per page.\n",
    "    \"\"\"\n",
    "    # Ensure both lists are of the same length\n",
    "    max_len = max(len(list1), len(list2))\n",
    "    list1.extend([np.zeros_like(list1[0])] * (max_len - len(list1)))\n",
    "    list2.extend([np.zeros_like(list2[0])] * (max_len - len(list2)))\n",
    "\n",
    "    # Create a PDF file\n",
    "    with PdfPages(output_pdf) as pdf:\n",
    "        for i in range(0, len(list1), pairs_per_page):\n",
    "            fig, axes = plt.subplots(\n",
    "                pairs_per_page, 2, figsize=(10, 5 * pairs_per_page)\n",
    "            )\n",
    "\n",
    "            for j in range(pairs_per_page):\n",
    "                if i + j < len(list1):\n",
    "                    # Display the first image\n",
    "                    axes[j, 0].imshow(\n",
    "                        list1[i + j], cmap=\"gray\" if list1[i + j].ndim == 2 else None\n",
    "                    )\n",
    "                    axes[j, 0].axis(\"off\")\n",
    "\n",
    "                    # Display the second image\n",
    "                    axes[j, 1].imshow(\n",
    "                        list2[i + j], cmap=\"gray\" if list2[i + j].ndim == 2 else None\n",
    "                    )\n",
    "                    axes[j, 1].axis(\"off\")\n",
    "                else:\n",
    "                    # Turn off unused subplots\n",
    "                    axes[j, 0].axis(\"off\")\n",
    "                    axes[j, 1].axis(\"off\")\n",
    "\n",
    "            # Adjust layout and add the figure to the PDF\n",
    "            plt.tight_layout()\n",
    "            pdf.savefig(fig)\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fca387ce-2656-48ef-b9ed-834ea60e2f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = {}\n",
    "\n",
    "with open(\"../training/data/data_sampled_w_duplications_shuffled.pkl\", \"rb\") as f:\n",
    "    X_data, Y_data = pk.load(f)\n",
    "\n",
    "for i, y in enumerate(Y_data):\n",
    "    if y in examples:\n",
    "        continue\n",
    "\n",
    "    examples[y] = X_data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feffaeb2-41d0-40d0-a1c1-da98188ca8b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.38 Î¼s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = \"../training/outputs/save_models/VideoCNN1_checkpoint_epoch20_augmented.pth\"\n",
    "# exps = random.choices(X_data, k=1)\n",
    "# predict_example_cuda(model, exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31c8b7cf-2222-4e28-a2b2-1b43a2213bf8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/d/gifs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m reddit_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/d/gifs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m reddit_data \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreddit_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m reddit_data \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(reddit_path, x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m reddit_data]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/d/gifs'"
     ]
    }
   ],
   "source": [
    "reddit_path = \"/d/gifs\"\n",
    "reddit_data = os.listdir(reddit_path)\n",
    "reddit_data = [os.path.join(reddit_path, x) for x in reddit_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06d843b-9e5e-4bb5-b3da-7ac91c5f642d",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = random.sample(reddit_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8be2a08-b6f8-4834-9857-f47a09b8cefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 5, figsize=(20, 8))\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    image = get_frame(samples[i])\n",
    "    ax.imshow(image)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2a2afd-d2bb-4118-a039-aff8944d487a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(2, 5, figsize=(20, 8))\n",
    "# predictions = predict_example(model, samples)\n",
    "\n",
    "# for i, ax in enumerate(axs.flat):\n",
    "#     prediction = predictions[i][1]\n",
    "#     predict_meme = examples[prediction]\n",
    "#     image = get_frame(predict_meme)\n",
    "#     ax.imshow(image)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435bce83-37e8-40a1-944a-c4080f373e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export pdf\n",
    "\n",
    "pdf_samples = random.sample(reddit_data, 100)\n",
    "og_images = [get_frame(s) for s in pdf_samples]\n",
    "\n",
    "predictions = predict_example(model, pdf_samples)\n",
    "predict_images = [get_frame(examples[p[1]]) if p[1] != 0 else None for p in predictions ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64331dc0-d9d4-4946-83dc-215ba742afd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_and_export_images(og_images, predict_images, \"./outputs/reddit_test.pdf\", pairs_per_page=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3a3b62-810b-4f38-ab2f-d843693756e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b74d16-871c-42ff-b84f-c13fb4c447a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9389be5-483b-4cd6-a9d6-1a3284547617",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
