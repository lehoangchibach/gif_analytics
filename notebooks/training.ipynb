{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87385720-1c89-4ac2-9297-8d52d7ac966e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af323646-bcf1-4643-a8dc-01eeee6dc1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPU devices: 2\n",
      "\n",
      "GPU properties:\n",
      "  Name: NVIDIA GeForce RTX 3060 Ti\n",
      "  Multi Processor Count: 38\n",
      "  Total memory: 7870 MB\n",
      "  Memory allocated: 0.00 MB\n",
      "  Memory cached: 0.00 MB\n",
      "\n",
      "GPU properties:\n",
      "  Name: NVIDIA GeForce GTX 1080 Ti\n",
      "  Multi Processor Count: 28\n",
      "  Total memory: 11165 MB\n",
      "  Memory allocated: 0.00 MB\n",
      "  Memory cached: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of GPU devices: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "\n",
    "        gpu_props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"\\nGPU properties:\")\n",
    "        print(f\"  Name: {gpu_props.name}\")\n",
    "        print(f\"  Multi Processor Count: {gpu_props.multi_processor_count}\")\n",
    "        print(f\"  Total memory: {gpu_props.total_memory / 1024**2:.0f} MB\")\n",
    "        print(f\"  Memory allocated: {torch.cuda.memory_allocated(i) / 1024**2:.2f} MB\")\n",
    "        print(f\"  Memory cached: {torch.cuda.memory_reserved(i) / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59f678f8-23ae-4863-99e4-2579f86e672a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20584,), (20584,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./data/data.pkl\", \"rb\") as f:\n",
    "    X_data, Y_data = pk.load(f)\n",
    "X_data.shape, Y_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba5697d-3fef-4fab-8420-0aed1967c378",
   "metadata": {},
   "source": [
    "# Pre-processing X data\n",
    "\n",
    "meme\n",
    "- gif: visual (5 frames) / audio (Null) / text (fr visual)\n",
    "- mp4: visual (5 frames) / audio (Null) / text (fr visual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d56025b-2194-427f-8920-c5816a6fd779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "NUM_FRAMES = 5\n",
    "IMAGE_SIZE = (512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfbaef9-e0e3-4c29-bbc5-0a69d8b017a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def input_processor(video_path: str) -> np.ndarray:\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get total number of frames\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    if total_frames <= 0:\n",
    "        raise ValueError(\"Could not read frames from video file\")\n",
    "\n",
    "    # Calculate frame indices to extract\n",
    "    frame_indices = [\n",
    "        i * (total_frames - 1) // (NUM_FRAMES - 1) for i in range(NUM_FRAMES)\n",
    "    ]\n",
    "\n",
    "    for frame_idx in frame_indices:\n",
    "        # Set frame position\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "\n",
    "        # Read frame\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            # Convert BGR to RGB\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame_rgb = cv2.resize(frame_rgb, (512, 512))\n",
    "\n",
    "            frames.append(frame_rgb)\n",
    "\n",
    "    # Release video capture object\n",
    "    cap.release()\n",
    "\n",
    "    while len(frames) < 5:\n",
    "        frames.append(frames[-1])\n",
    "\n",
    "    video_data = np.stack(frames) / 255.0\n",
    "    return video_data.astype(np.float32)\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, losses, accuracies):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (videos, labels) in enumerate(dataloader):\n",
    "        videos, labels = videos.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(videos)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        # Print progress\n",
    "        if batch_idx % 10 == 0:\n",
    "            loss = loss.item()\n",
    "            acc = 100.0 * correct / total\n",
    "            print(\n",
    "                f\"Batch: {batch_idx}/{len(dataloader)}, \"\n",
    "                f\"Loss: {loss:.4f}, \"\n",
    "                f\"Acc: {acc:.2f}%\"\n",
    "            )\n",
    "            losses.append(loss)\n",
    "            accuracies.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fad51f2-a700-4ab8-aea6-a21c1046db39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/lehoangchibach/anaconda3/envs/gif_analytic/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lehoangchibach/anaconda3/envs/gif_analytic/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'train_model_ddp' on <module '__main__' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "def input_processor(video_path: str) -> np.ndarray:\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get total number of frames\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    if total_frames <= 0:\n",
    "        raise ValueError(\"Could not read frames from video file\")\n",
    "\n",
    "    # Calculate frame indices to extract\n",
    "    frame_indices = [\n",
    "        i * (total_frames - 1) // (NUM_FRAMES - 1) for i in range(NUM_FRAMES)\n",
    "    ]\n",
    "\n",
    "    for frame_idx in frame_indices:\n",
    "        # Set frame position\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "\n",
    "        # Read frame\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            # Convert BGR to RGB\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame_rgb = cv2.resize(frame_rgb, (512, 512))\n",
    "\n",
    "            frames.append(frame_rgb)\n",
    "\n",
    "    # Release video capture object\n",
    "    cap.release()\n",
    "\n",
    "    while len(frames) < 5:\n",
    "        frames.append(frames[-1])\n",
    "\n",
    "    video_data = np.stack(frames) / 255.0\n",
    "    return video_data.astype(np.float32)\n",
    "\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        video_paths: List[str],\n",
    "        template_ids: List[int],\n",
    "        custom_processor: callable,\n",
    "    ):\n",
    "        self.video_paths = video_paths\n",
    "        # Convert template IDs to tensor immediately\n",
    "        self.template_ids = torch.tensor(template_ids, dtype=torch.long)\n",
    "        self.custom_processor = custom_processor\n",
    "\n",
    "        # Store unique template IDs for reference\n",
    "        self.unique_templates = torch.unique(self.template_ids)\n",
    "        self.num_classes = len(self.unique_templates)\n",
    "\n",
    "        # Create mapping from template ID to class index (0 to num_classes-1)\n",
    "        self.template_to_idx = {\n",
    "            int(tid): idx for idx, tid in enumerate(self.unique_templates)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        template_id = int(self.template_ids[idx])\n",
    "\n",
    "        video = self.custom_processor(video_path)\n",
    "        video_tensor = torch.from_numpy(video)\n",
    "\n",
    "        # Convert template ID to class index\n",
    "        class_idx = self.template_to_idx[template_id]\n",
    "\n",
    "        return video_tensor, torch.tensor(class_idx, dtype=torch.long)\n",
    "\n",
    "    def get_original_template_id(self, class_idx: int) -> int:\n",
    "        \"\"\"Convert back from class index to original template ID\"\"\"\n",
    "        return int(self.unique_templates[class_idx])\n",
    "\n",
    "\n",
    "class VideoCNN(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv3d = nn.Sequential(\n",
    "            nn.Conv3d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=2),\n",
    "            nn.Conv3d(32, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        # Calculate the size of the flattened features\n",
    "        self.flat_features = self._calculate_flat_features()\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.flat_features, 8), nn.ReLU(), nn.Linear(8, num_classes)\n",
    "        )\n",
    "\n",
    "    def _calculate_flat_features(self) -> int:\n",
    "        x = torch.randn(1, 3, NUM_FRAMES, 512, 512)\n",
    "        x = self.conv3d(x)\n",
    "        return int(np.prod(x.shape[1:]))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Expected input shape: (batch_size, num_frames, height, width, channels)\n",
    "        # Need to permute to: (batch_size, channels, num_frames, height, width)\n",
    "        x = x.permute(0, 4, 1, 2, 3).contiguous()\n",
    "\n",
    "        x = self.conv3d(x)\n",
    "        # print(f\"Shape after conv3d: {x.shape}\")\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # print(f\"Shape after flatten: {x.shape}\")\n",
    "\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def setup_ddp(rank, world_size):\n",
    "    \"\"\"\n",
    "    Setup for distributed training\n",
    "    \"\"\"\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"12355\"\n",
    "\n",
    "    # Initialize the process group\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "    # Set device for this process\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "\n",
    "def cleanup_ddp():\n",
    "    \"\"\"\n",
    "    Clean up distributed training\n",
    "    \"\"\"\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "def train_model_ddp(rank, world_size: int, dataset, num_epochs=1):\n",
    "    # Setup DDP\n",
    "    setup_ddp(rank, world_size)\n",
    "\n",
    "    # Create dataloader\n",
    "    train_sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)\n",
    "    num_workers = 4 * world_size\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        sampler=train_sampler,\n",
    "    )\n",
    "\n",
    "    # Create model and move it to GPU\n",
    "    model = VideoCNN(num_classes=dataset.num_classes).to(rank)\n",
    "    model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    # Criterion & Optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_sampler.set_epoch(epoch)  # Important for proper shuffling\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_idx, (videos, labels) in enumerate(dataloader):\n",
    "            videos = videos.to(rank)\n",
    "            labels = labels.to(rank)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(videos)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            if batch_idx % 10 == 0 and rank == 0:\n",
    "                print(\n",
    "                    f\"Epoch: {epoch}, Batch: {batch_idx}, \"\n",
    "                    f\"Loss: {loss.item():.4f}, \"\n",
    "                    f\"Acc: {100.0 * correct / total:.2f}%\"\n",
    "                )\n",
    "\n",
    "        if rank == 0:\n",
    "            epoch_loss = running_loss / len(dataloader)\n",
    "            accuracy = 100.0 * correct / total\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "                f\"Loss: {epoch_loss:.4f}, \"\n",
    "                f\"Accuracy: {accuracy:.2f}%\"\n",
    "            )\n",
    "\n",
    "    cleanup_ddp()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    dataset = VideoDataset(X_data, Y_data, custom_processor=input_processor)\n",
    "\n",
    "    # Create model with correct number of output classes\n",
    "    # model = VideoCNN(num_classes=dataset.num_classes)\n",
    "    # Training setup\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # model = model.to(device)\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # losses = []\n",
    "    # accuracies = []\n",
    "    # train_epoch(model, dataloader, criterion, optimizer, losses, accuracies)\n",
    "\n",
    "    world_size = torch.cuda.device_count()\n",
    "    mp.set_start_method(\"spawn\", force=True)\n",
    "    mp.spawn(\n",
    "        train_model_ddp,\n",
    "        args=(world_size, dataset, 1),\n",
    "        nprocs=world_size,\n",
    "        join=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad19955-779a-441f-9f5e-dc8802437614",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
